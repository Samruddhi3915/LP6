{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004baada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4351af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello, I am Samruddhi from BE3.I am pursuing my engineering from computer technology from Pune Institute of Computer Technology.I am in P3 batch currently and this are my NLP assignments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba6e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'I', 'am', 'Samruddhi', 'from', 'BE3.I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology.I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments']\n"
     ]
    }
   ],
   "source": [
    "#WhitespaceTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "whitespace_tokenized = WhitespaceTokenizer().tokenize(sentence)\n",
    "print(whitespace_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eeea46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3.I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology.I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments']\n"
     ]
    }
   ],
   "source": [
    "#TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenized = TreebankWordTokenizer().tokenize(sentence)\n",
    "print(treebank_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09318c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'I', ' ', 'a', 'm', ' ', 'S', 'a', 'm', 'r', 'u', 'd', 'd', 'h', 'i', ' ', 'f', 'r', 'o', 'm', ' ', 'B', 'E', '3', '.', 'I', ' ', 'a', 'm', ' ', 'p', 'u', 'r', 's', 'u', 'i', 'n', 'g', ' ', 'm', 'y', ' ', 'e', 'n', 'g', 'i', 'n', 'e', 'e', 'r', 'i', 'n', 'g', ' ', 'f', 'r', 'o', 'm', ' ', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 't', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', ' ', 'f', 'r', 'o', 'm', ' ', 'P', 'u', 'n', 'e', ' ', 'I', 'n', 's', 't', 'i', 't', 'u', 't', 'e', ' ', 'o', 'f', ' ', 'C', 'o', 'm', 'p', 'u', 't', 'e', 'r', ' ', 'T', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', '.', 'I', ' ', 'a', 'm', ' ', 'i', 'n', ' ', 'P', '3', ' ', 'b', 'a', 't', 'c', 'h', ' ', 'c', 'u', 'r', 'r', 'e', 'n', 't', 'l', 'y', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'i', 's', ' ', 'a', 'r', 'e', ' ', 'm', 'y', ' ', 'N', 'L', 'P', ' ', 'a', 's', 's', 'i', 'g', 'n', 'm', 'e', 'n', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "#MWETokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer([('he'),('take','a','good')])\n",
    "mwe_tokenizer.add_mwe(('Harry'))\n",
    "mwe_tokenized = mwe_tokenizer.tokenize(sentence)\n",
    "print(mwe_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7999f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments']\n"
     ]
    }
   ],
   "source": [
    "#TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenized = TweetTokenizer().tokenize(sentence)\n",
    "print(tweet_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d974f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments']\n"
     ]
    }
   ],
   "source": [
    "#Punctuation based word tokenization\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37bacee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I am Samruddhi from BE3.I am pursuing my engineering from computer technology from Pune Institute of Computer Technology.I am in P3 batch currently and this are my NLP assignments']\n"
     ]
    }
   ],
   "source": [
    "#Punctuation based sentence tokenization\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sent_tokenized = sent_detector.tokenize(sentence)\n",
    "print(sent_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27b4a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Snowball Stemming\n",
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments'] \n",
      "\n",
      "After Snowball Stemming\n",
      "['hello', ',', 'i', 'am', 'samruddhi', 'from', 'be3', '.', 'i', 'am', 'pursu', 'my', 'engin', 'from', 'comput', 'technolog', 'from', 'pune', 'institut', 'of', 'comput', 'technolog', '.', 'i', 'am', 'in', 'p3', 'batch', 'current', 'and', 'this', 'are', 'my', 'nlp', 'assign']\n"
     ]
    }
   ],
   "source": [
    "#SnowballStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowballstemmer = SnowballStemmer(language='english')\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = snowballstemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Snowball Stemming')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Snowball Stemming')\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5e33271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Potter Stemming\n",
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments'] \n",
      "\n",
      "After Potter Stemming\n",
      "['hello', ',', 'i', 'am', 'samruddhi', 'from', 'be3', '.', 'i', 'am', 'pursu', 'my', 'engin', 'from', 'comput', 'technolog', 'from', 'pune', 'institut', 'of', 'comput', 'technolog', '.', 'i', 'am', 'in', 'p3', 'batch', 'current', 'and', 'thi', 'are', 'my', 'nlp', 'assign']\n"
     ]
    }
   ],
   "source": [
    "#PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "porterstemmer = PorterStemmer()\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = porterstemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Potter Stemming')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Potter Stemming')\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d09cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization\n",
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignments'] \n",
      "\n",
      "After Lemmatization\n",
      "['Hello', ',', 'I', 'am', 'Samruddhi', 'from', 'BE3', '.', 'I', 'am', 'pursuing', 'my', 'engineering', 'from', 'computer', 'technology', 'from', 'Pune', 'Institute', 'of', 'Computer', 'Technology', '.', 'I', 'am', 'in', 'P3', 'batch', 'currently', 'and', 'this', 'are', 'my', 'NLP', 'assignment']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stem_words = []\n",
    "for w in tweet_tokenized:\n",
    "    x = wordnet_lemmatizer.lemmatize(w)\n",
    "    stem_words.append(x)\n",
    "print('Before Lemmatization')\n",
    "print(tweet_tokenized,'\\n')\n",
    "\n",
    "print('After Lemmatization')\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b387de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ccd39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78525a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd710b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8390e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d5455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f6a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef1c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a963f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
